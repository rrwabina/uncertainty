{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def init_params(net):\n",
    "    '''Init layer parameters.'''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal_(m.weight, mode = 'fan_out')\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant_(m.weight, 1)\n",
    "            init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal_(m.weight, std=1e-3)\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "def norm(dim):\n",
    "    return nn.GroupNorm(min(32, dim), dim)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = torch.prod(torch.tensor(x.shape[1: ])).item()\n",
    "        return x.view(-1, shape)\n",
    "\n",
    "class ConcatConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, ksize = 3, stride = 1, padding = 0, dilation = 1, groups = 1, bias = True, transpose = False):\n",
    "        super(ConcatConv2d, self).__init__()\n",
    "        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n",
    "        self._layer = module(\n",
    "            dim_in + 1, dim_out, kernel_size = ksize, stride = stride, padding = padding, dilation = dilation, groups = groups,\n",
    "            bias = bias\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        tt = torch.ones_like(x[:, :1, :, :]) * t\n",
    "        ttx = torch.cat([tt, x], 1)\n",
    "        return self._layer(ttx)\n",
    "\n",
    "\n",
    "class Drift(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Drift, self).__init__()\n",
    "        self.norm1 = norm(dim)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
    "        self.norm2 = norm(dim)\n",
    "        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
    "        self.norm3 = norm(dim)\n",
    "    \n",
    "    def forward(self, t, x):\n",
    "        out = self.norm1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(t, out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(t, out)\n",
    "        out = self.norm3(out)\n",
    "        return out\n",
    "\n",
    "class Diffusion(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(Diffusion, self).__init__()\n",
    "        self.norm1 = norm(dim_in)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.conv1 = ConcatConv2d(dim_in, dim_out, 3, 1, 1)\n",
    "        self.norm2 = norm(dim_in)\n",
    "        self.conv2 = ConcatConv2d(dim_in, dim_out, 3, 1, 1)\n",
    "        self.fc = nn.Sequential(norm(dim_out), nn.ReLU(inplace = True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(dim_out, 1), nn.Sigmoid())\n",
    "    def forward(self, t, x):\n",
    "        out = self.norm1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(t, out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(t, out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class SDENet(nn.Module):\n",
    "    def __init__(self, layer_depth, num_classes = 10, dim = 64):\n",
    "        super(SDENet, self).__init__()\n",
    "        self.layer_depth = layer_depth \n",
    "        self.downsampling_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, dim, 3, 1),\n",
    "            norm(dim),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(dim, dim, 4, 2),\n",
    "            norm(dim),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(dim, dim, 4, 2)\n",
    "        )\n",
    "        self.drift = Drift(dim)\n",
    "        self.diffusion = Diffusion(dim, dim)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            norm(dim),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(dim, 10)\n",
    "        )\n",
    "        self.deltat = 6./self.layer_depth\n",
    "        self.apply(init_params)\n",
    "        self.sigma = 500\n",
    "    \n",
    "    def forward(self, x, training_diffusion = False):\n",
    "        out = self.downsampling_layers(x)\n",
    "        if not training_diffusion:\n",
    "            t = 0\n",
    "            diffusion_term = self.sigma * self.diffusion(t, out)\n",
    "            diffusion_term = torch.unsqueeze(diffusion_term, 2)\n",
    "            diffusion_term = torch.unsqueeze(diffusion_term, 3)\n",
    "            for i in range(self.layer_depth):\n",
    "                t = 6 * (float(i))/self.layer_depth\n",
    "                out = out + self.drift(t, out) * self.deltat + diffusion_term * math.sqrt(self.deltat) * torch.randn_like(out).to(x)\n",
    "            final_out = self.fc_layers(out)\n",
    "        else:\n",
    "            t = 0\n",
    "            final_out = self.diffusion(t, out.detach())\n",
    "        return final_out\n",
    "\n",
    "def test():\n",
    "    model = SDENet(layer_depth = 10, num_classes = 10, dim = 64)\n",
    "    return model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = test()\n",
    "    num_params = count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building MNIST data loader with 1 workers\n"
     ]
    }
   ],
   "source": [
    "def getMNIST(batch_size, test_batch_size, img_size, **kwargs):\n",
    "    num_workers = kwargs.setdefault('num_workers', 1)\n",
    "    kwargs.pop('input_size', None)\n",
    "    print(\"Building MNIST data loader with {} workers\".format(num_workers))\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    ds = []\n",
    "    train_loader = DataLoader(\n",
    "        datasets.MNIST(root='../data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\n",
    "        shuffle=True, num_workers=num_workers, drop_last=True\n",
    "    )\n",
    "    ds.append(train_loader)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        datasets.MNIST(root='../data/mnist', train=False, download=True, transform=transform_test),\n",
    "        batch_size=test_batch_size, shuffle=False, num_workers=num_workers, drop_last=True\n",
    "    )\n",
    "    ds.append(test_loader)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def getDataSet(data_type, batch_size, test_batch_size, imageSize):\n",
    "    if data_type == 'svhn':\n",
    "        train_loader, test_loader = getSVHN(batch_size, test_batch_size, imageSize)\n",
    "    elif data_type == 'mnist':\n",
    "        train_loader, test_loader = getMNIST(batch_size, test_batch_size, imageSize)\n",
    "    elif data_type == 'semeion':\n",
    "        train_loader, test_loader = getSEMEION(batch_size, test_batch_size, imageSize)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = getDataSet('mnist', batch_size = 128, test_batch_size = 125, imageSize = 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load in-domain data:  mnist\n",
      "Building MNIST data loader with 1 workers\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import argparse\n",
    "\n",
    "device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('load in-domain data: ', 'mnist')\n",
    "train_loader_inDomain, test_loader_inDomain = getDataSet('mnist', 128, 128, 28)\n",
    "\n",
    "net = SDENet(layer_depth = 6, num_classes = 10, dim = 64)\n",
    "net = net.to(device)\n",
    "\n",
    "real_label = 0\n",
    "fake_label = 1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.BCELoss()\n",
    "\n",
    "optimizer_F = optim.SGD([ {'params': net.downsampling_layers.parameters()}, {'params': net.drift.parameters()},\n",
    "{'params': net.fc_layers.parameters()}], lr = 0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "optimizer_G = optim.SGD([ {'params': net.diffusion.parameters()}], lr = 0.1, momentum=0.9, weight_decay=5e-4)\n",
    "net.sigma = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer_train = SummaryWriter('runs/sdenet/train')\n",
    "writer_test = SummaryWriter('runs/sdenet/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss_out = 0\n",
    "    train_loss_in = 0\n",
    "\n",
    "    ##training with in-domain data\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader_inDomain):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer_F.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss = loss.to(torch.float32)\n",
    "        loss.backward()\n",
    "        optimizer_F.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    #training with out-of-domain data\n",
    "        label = torch.full((128, 1), real_label, device=device)\n",
    "        optimizer_G.zero_grad()\n",
    "        predict_in = net(inputs, training_diffusion = True)\n",
    "        predict_in = predict_in.to(torch.float32)\n",
    "        label = label.to(torch.float32)\n",
    "        loss_in = criterion2(predict_in, label)\n",
    "        loss_in.backward()\n",
    "        label.fill_(fake_label)\n",
    "        inputs_out = 2*torch.randn(128, 1, 28, 28, device = device) + inputs\n",
    "        predict_out = net(inputs_out, training_diffusion=True)\n",
    "        loss_out = criterion2(predict_out, label)\n",
    "        loss_out.backward()\n",
    "        train_loss_out += loss_out.item()\n",
    "        train_loss_in += loss_in.item()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        writer_train.add_scalar(loss_out, epoch)\n",
    "\n",
    "    print('Train epoch:{} \\tLoss: {:.6f} | Loss_in: {:.6f}, Loss_out: {:.6f} | Acc: {:.6f} ({}/{})'\n",
    "        .format(epoch, train_loss/(len(train_loader_inDomain)), train_loss_in/len(train_loader_inDomain), train_loss_out/len(train_loader_inDomain), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader_inDomain):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = 0\n",
    "            for j in range(5):\n",
    "                current_batch = net(inputs)\n",
    "                outputs = outputs + F.softmax(current_batch, dim = 1)\n",
    "\n",
    "            outputs = outputs/5\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print('Test epoch: {} | Acc: {:.6f} ({}/{})'\n",
    "        .format(epoch, 100.*correct/total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
